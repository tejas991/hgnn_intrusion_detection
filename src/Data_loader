# src/data_loader.py
"""
Data loading utilities for NSL-KDD dataset
"""

import os
import gc
from sklearn.model_selection import train_test_split
from .data_processor import MemoryEfficientNSLKDD


class NSLKDDLoader:
    """NSL-KDD dataset loader with automatic file detection and processing"""
    
    def __init__(self, data_dir="data/raw"):
        self.data_dir = data_dir
        self.processor = MemoryEfficientNSLKDD()
        
    def setup_data_directory(self):
        """Create data directory structure"""
        os.makedirs(self.data_dir, exist_ok=True)
        print(f"Data directory created: {self.data_dir}")
        
    def scan_available_files(self):
        """Scan and detect available NSL-KDD files"""
        if not os.path.exists(self.data_dir):
            print(f"âŒ Data directory not found: {self.data_dir}")
            return []
            
        raw_files = os.listdir(self.data_dir)
        print("Files detected:", raw_files)
        return raw_files
    
    def detect_training_file(self, raw_files):
        """Automatically detect the best training file to use"""
        if any('20Percent' in f for f in raw_files):
            # Use the smaller version for faster training
            train_filename = [f for f in raw_files if '20Percent' in f][0]
            max_train_samples = 30000
            print(f"ğŸ“Š 20% training file detected: '{train_filename}' (using up to {max_train_samples} samples)")
        else:
            # Full version
            train_filename = [f for f in raw_files if 'Train' in f and '20Percent' not in f][0]
            max_train_samples = 25000
            print(f"ğŸ“Š Full training file: '{train_filename}' (capped at {max_train_samples} samples)")
            
        return train_filename, max_train_samples
    
    def detect_test_file(self, raw_files):
        """Detect the test file"""
        test_files = [f for f in raw_files if 'Test' in f]
        if not test_files:
            raise FileNotFoundError("âŒ No test file found. Expected file with 'Test' in the name.")
        
        test_filename = test_files[0]
        max_test_samples = 15000
        print(f"ğŸ“Š Test file detected: '{test_filename}' (using up to {max_test_samples} samples)")
        return test_filename, max_test_samples
    
    def load_and_split_data(self, validation_split=0.2, random_state=42):
        """Complete data loading and splitting pipeline"""
        print("\nğŸ”„ Loading NSL-KDD Dataset...")
        
        # Setup and scan
        self.setup_data_directory()
        raw_files = self.scan_available_files()
        
        if not raw_files:
            print("\nâŒ No data files found!")
            print("ğŸ“¥ Please download NSL-KDD dataset files and place them in:", self.data_dir)
            print("   Required files:")
            print("   - KDDTrain+.txt (or KDDTrain+_20Percent.txt)")
            print("   - KDDTest+.txt")
            return None
        
        # Detect files
        try:
            train_filename, max_train_samples = self.detect_training_file(raw_files)
            test_filename, max_test_samples = self.detect_test_file(raw_files)
        except (IndexError, FileNotFoundError) as e:
            print(f"âŒ Error detecting files: {e}")
            return None
        
        # Load training data
        print(f"\nğŸ“‚ Loading training set from: {train_filename}")
        train_data_path = os.path.join(self.data_dir, train_filename)
        X_train_all, y_train_all = self.processor.process_data_smart(
            train_data_path,
            is_training=True,
            max_samples=max_train_samples
        )
        print("âœ… Training set loaded!")
        
        # Load test data
        print(f"\nğŸ“‚ Loading test set from: {test_filename}")
        test_data_path = os.path.join(self.data_dir, test_filename)
        X_test, y_test = self.processor.process_data_smart(
            test_data_path,
            is_training=False,
            max_samples=max_test_samples
        )
        print("âœ… Test set loaded!")
        
        # Display shapes
        print(f"\nğŸ“Š Data shapes:")
        print(f"   Training data: {X_train_all.shape}")
        print(f"   Test data: {X_test.shape}")
        
        # Split training into train/validation
        print(f"\nğŸ”„ Splitting training data (validation split: {validation_split})")
        X_train, X_val, y_train, y_val = train_test_split(
            X_train_all, y_train_all,
            test_size=validation_split,
            stratify=y_train_all,
            random_state=random_state
        )
        
        print(f"âœ… Data split complete:")
        print(f"   Training set: {X_train.shape}")
        print(f"   Validation set: {X_val.shape}")
        print(f"   Test set: {X_test.shape}")
        
        # Memory cleanup
        del X_train_all, y_train_all
        gc.collect()
        print("ğŸ§¹ Memory cleanup completed")
        
        return {
            'X_train': X_train,
            'X_val': X_val,
            'X_test': X_test,
            'y_train': y_train,
            'y_val': y_val,
            'y_test': y_test,
            'processor': self.processor
        }


def load_nslkdd_data(data_dir="data/raw", validation_split=0.2, random_state=42):
    """Convenience function to load NSL-KDD data"""
    loader = NSLKDDLoader(data_dir)
    return loader.load_and_split_data(validation_split, random_state)


# For Google Colab users
def upload_data_colab():
    """
    Special function for Google Colab users to upload data files
    Note: This only works in Google Colab environment
    """
    try:
        from google.colab import files
        
        print("ğŸ“¤ Google Colab file upload:")
        print("Please upload the NSL-KDD files:")
        print("- Prefer 'KDDTrain+_20Percent.txt' over full 'KDDTrain+.txt'")
        print("- You'll need both: 'KDDTrain+.txt' and 'KDDTest+.txt'")
        
        uploaded_files = files.upload()
        
        # Organize in raw folder
        os.makedirs("data/raw", exist_ok=True)
        
        for fname in uploaded_files:
            original_path = fname
            target_path = f"data/raw/{fname}"
            os.rename(original_path, target_path)
            size_in_mb = os.path.getsize(target_path) / (1024 * 1024)
            print(f"âœ… Saved {fname} to {target_path} ({size_in_mb:.2f} MB)")
            
        print("âœ… File upload completed!")
        return True
        
    except ImportError:
        print("âŒ This function only works in Google Colab!")
        print("ğŸ“¥ Please manually download and place NSL-KDD files in 'data/raw/' folder")
        return False
